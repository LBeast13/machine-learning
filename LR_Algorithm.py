# -*- coding: utf-8 -*-
"""
Created on Sun Nov  4 03:17:58 2018

@author: Liam
"""

import numpy as np
import matplotlib.pyplot as plt
import PLA_Algorithm as pla

    
def prepare_data(nb_points):
    """
    Prepare the dataset (training points) 
    We have a target function (only here to visualize because it is usually unknown)
    It also display the Graph
    
    Return weights, x_n, y_n, the slope and the intercept of the target function
    """
    
    #TARGET FUNCTION
    slope_target, intercept_target = target_function_generation()
    
    #TRAINING POINTS
    x_n, y_n = creation_dataset(nb_points, slope_target, intercept_target)
    
    return x_n, y_n, slope_target, intercept_target

def target_function_generation():
    """ 
    Generate the target function with 2 random uniform points and using the 
    slope and the intercept of the line.
    
    Returns a (the slope) and b(the intercept)
    """

    #Random points in [-1,1]x[-1,1]
    point_1 = [np.random.uniform(-1,1,1),np.random.uniform(-1,1,1)]
    point_2 = [np.random.uniform(-1,1,1),np.random.uniform(-1,1,1)]

    #TARGET FUNCTION SLOPE AND INTERCEPT
    a = (point_2[1]-point_1[1])/(point_2[0]-point_1[0]) #slope
    b = point_1[1]-(a*point_1[0]) #intercept 
    
    return a, b


def creation_dataset(nb_points, slope, intercept):
    """
    Generates the dataset of nbPoints (x_1X,x_1Y,y_1) using random uniform distribution.
    We use the equation of the target function to get the y_n:
    if the point is under the line is -1, and 1 if it is over.
    
    Returns x_n and y_n, the training points
    """
    
    #DATA POINTS    
    x_n = []
    
    for j in range(nb_points):
        x_curr = np.random.uniform(-1,1,2)
        
        #The w0 in order to make the matricial product
        #We will always have (x1,x2,1)
        x_curr = np.append(x_curr,1) 
        x_n.append(x_curr)

    x_n = np.array(x_n)
    
    #CALCULATING THE Y of the X DATA POINTS
    y_n = calculate_y_n(nb_points, x_n, slope, intercept)
   
    return x_n, y_n
    

def compare(nb_points, y_n, y_n_learn):
    """
    Compares the y_n targets of the training point with the y obtained using
    the learned hypothesis on the same training points.
    Used to calculate E_in and E_out
    """
    
    misclassified_count = 0 #number of misclassified points
    
    for i in range(nb_points):
        if y_n[i] != y_n_learn[i]:
            misclassified_count += 1
    
    E_in = misclassified_count/nb_points
    
    return E_in


def error_in_out_sample(nb_points, nb_test_points, x_n, y_n, 
                        slope_hypot, intercept_hypot, 
                        slope_target, intercept_target):
    """
    Calculates E_in (error in sample) and E_out (error out of sample)
    E_in = the fraction of in-sample points which got classified incorrectly
    E_out = number of misclassified out-of-sample points / number of out-of-sample points)
    
    Returns E_in and E_out
    """
    #CALCULATING THE Y OF THE TRAINING POINTS USING OUR HYPOTHESIS (LEARNED) 
    
    #DATASET FOR OUT OF SAMPLE 
    x_n_out, y_n_out = creation_dataset(nb_test_points, slope_target, 
                                        intercept_target)
    
    #Y_N CALCULATED USING THE FINAL HYPOTHESIS
    y_n_learn_in = calculate_y_n(nb_points, x_n, slope_hypot, intercept_hypot)
    y_n_learn_out = calculate_y_n(nb_test_points,x_n_out, slope_hypot, intercept_hypot)
    
    #COMPARE THE Y TARGET AND THE Y GENERATED BY OUR HYPOTHESIS
    E_in = compare(nb_points, y_n, y_n_learn_in)
    E_out = compare(nb_test_points, y_n_out, y_n_learn_out)
    
    return E_in, E_out  


def calculate_y_n(nb_points, x_n, slope_hypot, intercept_hypot):
    """
    Calculates and returns the y_n using our final hypothesis 
    """
    
    y_n_learn = [] #targets using hypothesis

    for i in range(nb_points):
        if slope_hypot * x_n[i,0] + intercept_hypot <= x_n[i,1]:
            y_n_learn.append(1)
        else:
            y_n_learn.append(-1)

    y_n_learn = np.array(y_n_learn)
    
    return y_n_learn


def display_experience(nb_points, x_n, y_n, slope_hypot, intercept_hypot,
                       slope_target, intercept_target, weights):
    """
    Displays the experience: 
    target function (green)
    hypothesis learned (yellow)
    training points
    """
    
    #GRAPH WINDOW
    axes = plt.gca()
    axes.set_xlim(-1, 1) #MIN and MAX values on X axe 
    axes.set_ylim(-1, 1) #MIN and MAX values on Y axe
    
    #DISPLAYS THE TRAINING POINTS
    #   RED : y = +1 or 0
    #   YELLOW : y = -1
    for i in range(nb_points):
        plt.plot(x_n[i,0],x_n[i,1],'ro' if (y_n[i] >= 1) else 'bo')
            
    #POINTS TO DISPLAY THE TARGET FUNCTION
    x_target = np.linspace(-1.,1.,100) #100 random points between -1 and 1
    y_Target = slope_target * x_target + intercept_target #y associated to the 100 points
    
    plt.plot(x_target,y_Target,'g-') #Display the line of the target function
    
    
    #DISPLAY OF THE HYPOTHESIS (LEARNING)
    x_hypot = np.linspace(-1.,1.,100)

    #y = ax + b, a is slope and b is intercept
    y_hypot = (slope_hypot * x_hypot) + intercept_hypot
    plt.plot(x_hypot, y_hypot,'y-')
    
    plt.show()


def linear_regression(nb_points, x_n, y_n):
    """
    THE LINEAR REGRESSION ALGORITHM
    
    Uses the pseudo inverse of the training points, then makes the matricial product
    to calculate the final weights.
    This is a one-step learning algorithm
    
    Returns the slope, intercept of the final hypothesis and the weights
    """
    
    #PSEUDO INVERSE
    pseudo_inverse_xn = np.linalg.pinv(x_n)
    
    #WEIGHTS 
    weights = np.dot(pseudo_inverse_xn, y_n)
    
    #We have the equation: w[2] + w[1]*y + w[0]*x = 0
    slope_hypot = -(weights[0])/(weights[1])  
    intercept_hypot = -weights[2]/weights[1]

    return slope_hypot, intercept_hypot, weights
  

if __name__ == '__main__':
 
    #NUMBER OF TRAINING POINTS (DATASET)
    nb_points = 10 
    
    #NUMBER OF TEST POINTS TO CALCULATE E_OUT
    nb_test_points = 1000
    
    #NUMBER OF INDEPENDENT EXPERIENCES THAT WILL BE REALISED
    nb_experiences = 1
    

    sum_E_in = 0
    sum_E_out = 0
    sum_iter = 0
    
    #RUN MULTIPLE EXPERIENCES
    for i in range(nb_experiences):
        
        #PREPARE THE DATASET
        x_train_pts, y_train_pts, slope_target, intercept_target = prepare_data(nb_points)
   
        #RUN THE LINEAR REGRESSION ALGORITHM ON THE DATA
        slope_hypot, intercept_hypot, weights = linear_regression(nb_points,
                                                                  x_train_pts, 
                                                                  y_train_pts)
        
        #RUN THE PERCEPTRON LEARNING ALGORITHM USING THE WEIGHTS OBTAINED
        slope_hypot_pla, intercept_hypot_pla, proba_misclas_pla, nb_iter_pla = pla.perceptron(nb_points, 
                                                                                              x_train_pts, 
                                                                                              y_train_pts, 
                                                                                              weights,
                                                                                              nb_test_points,
                                                                                              slope_target,
                                                                                              intercept_target)
        
        #CALCULATE E_IN AND E_OUT
        E_in_curr, E_out_curr = error_in_out_sample(nb_points, nb_test_points,
                                                    x_train_pts, y_train_pts,
                                                    slope_hypot, intercept_hypot,
                                                    slope_target, intercept_target)
        
        #UPDATE COUNTERS
        sum_E_in += E_in_curr
        sum_E_out += E_out_curr
        sum_iter += nb_iter_pla
        
        #DISPLAY EACH 100 EXPERIENCES
        if (i+1)%100 == 0:
            print("{} experiences already done.".format(i+1))
            

    #CALCULATE THE AVERAGES
    E_in_avg = sum_E_in / nb_experiences
    E_out_avg = sum_E_out / nb_experiences
    iter_avg = sum_iter / nb_experiences
    
    #DISPLAY THE FINAL HYPOTHESIS
    display_experience(nb_points, x_train_pts, y_train_pts, slope_hypot, 
                       intercept_hypot, slope_target, intercept_target,
                       weights)
    
    #DISPLAY THE RESULT
    print("E_in average : {}".format(E_in_avg))
    print("E_out average : {}".format(E_out_avg))
    print("Nombre d'it√©rations : {}".format(iter_avg))
    
  